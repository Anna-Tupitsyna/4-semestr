1) KNN - это алгоритм "обучения с учителем", использующийся в основном для классификации или регрессии. Он наиболее эффективен для классификаций, в которых алгоритм определяет к какому классификационному признаку отнести исходную точку путем определения K-ближайщих точек. KNN классифицирует точку данных, базируясь на известной классификации других точек (на базе тренировочных данных).

KNN простой алгоритм, работающий на базе пространственной близости/расстояния и не нуждается в построении модели, поскольку он настраивает несколько параметров и делает предположения. KNN универсален, но становится медленнее по мере увеличения признаков.
# Создание классификатора
classifierKNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
# Обучение классификатора на тренировочных данных
classifierKNN.fit(X_train.values, y_train.values)
KNeighborsClassifier()
Метрика minkowski здесь означает MinkowskiDistance, которая рассчитывается по формуле как сумма ( | x - y | ^ p) ^( 1/p )
# Применение классификации к тестовым данным.
y_pred = classifierKNN.predict(X_test.values)
# Результат предсказания покупок на тестовых данных.
y_pred
array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1], dtype=int8)
y_test.values
array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1], dtype=int8)
# Оценка результатов KNN классификации
# Смотрим матрицу ошибок.
cm1 = confusion_matrix(y_test,y_pred)
print(cm1)
  [[63  5]
  [ 2 30]]
# Она же, но графически.
sns.heatmap(cm1, annot=True, fmt='d', cmap='Spectral')
plt.show()
Из 100 значений предсказаний, только 7 ошибочных.
Отчет о классификации
print(classification_report(y_test, y_pred))
Общая точность (accuracy) составляет 93%, а точность (precision), полнота (recall) и F1-мера для предсказания "не покупок" выше 94%, а для проноза покупок близка к 90%.

# значение ошибки.
np.mean(y_pred != y_test)
0.07
Выбор значения К:
error_rate = []

for i in range(1,40):
    # создание KNN классификатора с заданным значением k
    knn = KNeighborsClassifier(n_neighbors = i)
    # обучение его на тренировочных данных
    knn.fit(X_train.values, y_train.values)
    # прогноз покупок на тестовых данных
    pred_i = knn.predict(X_test.values)
    # сохранение значения ошибки
    error_rate.append(np.mean(pred_i != y_test))
2) Метод опорных векторов (SVM, Support vector machines) использует гиперплоскость, чтобы классифицировать данные по 2 классам.
Метод опорных векторов - это метод "обучения с учителем", использующий классификацию, регрессию и определение выбросов (outlier). Главное преимущество SVM заключается в том, что он эффективен в многомерном пространстве, даже когда количество выборок меньше количества измерений. Но в случаях, когда измерения превышают количество образцов, есть вероятность переобучения.
Основная цель алгоритма SVM - создать оптимальную линию или границу решения, представляющую собой гиперплоскость, которая может разделить n-мерное пространство на классы, чтобы в будущем можно было поместить новую точку данных в правильную категорию.
В отличие от других классификаторов, которые обращают внимание на все точки, этот метод сосредотачиваются только на точках, которые труднее всего классифицировать.
Гиперплоскость - это подпространство, размерность которого на единицу меньше, чем размер его окружающего пространства или пространства, окружающего объект. Если пространство трехмерно, то его гиперплоскости являются двумерными,
двумерно, его гиперплоскости являются одномерными линиями,
одномерное, его гиперплоскость представляет собой единственную точку.
SVM использует набор математических функций, известных как ядро, для создания границы оптимального решения, принимая данные в качестве входных. Наиболее предпочтительный вид функции ядра - это RBF. Потому что он локализован и имеет конечный отклик по всей оси абсцисс.
Если данные слишком похожи или их сложно разделить по какой-либо причине, или из-за того, что они нелинейны, тогда SVM с ядром может добавить еще одно измерение с «гиперплоскостью», которая может разделить точки данных.
Значениями ядра могут быть:
linear
poly
rbf (по умолчанию)
sigmoid
precomputed
SVM с линейным ядром
Это ядро лучше всего подходит для задач классификации, когда данные разделены линейно, как задача классификации текста.

# обучение модели
classifierLin = SVC(kernel = 'linear')
classifierLin.fit(X_train, y_train)
print(classifierLin.gamma)
print(classifierLin.C)
  scale
  1.0
# проверка на тестовых данных
y_pred_svc = classifierLin.predict(X_test)
y_pred_svc
array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1], dtype=int8)
# анализ результатов
# матрица ошибок
cm2 = confusion_matrix(y_test, y_pred_svc)
print(cm2)
 [[66  2]
 [10 22]]
sns.heatmap(cm2, annot=True, fmt='d', cmap='Spectral')
plt.show()
11 неверных прогнозов из 100.
print(classification_report(y_test, y_pred_svc))
Точность 88%, но полнота для совершенных покупок всего 69%.

SVM с ядром RBF (Radial Basis Function)
# обучение модели
classifierrbf = SVC(kernel = 'rbf')
classifierrbf.fit(X_train, y_train)
print(classifierLin.gamma)
print(classifierLin.C)
  scale
  1.0
# проверка на тестовых данных
y_pred_rbf = classifierrbf.predict(X_test)
y_pred_rbf
array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1], dtype=int8)
# анализ результатов
# матрица ошибок
cm3 = confusion_matrix(y_test, y_pred_rbf)
print(cm3)
 [[64  4]
 [ 3 29]]
7 неверных прогнозов из 100.
print(classification_report(y_test,y_pred_rbf))
Общая точность (accuracy) составляет 93%, а точность (precision), полнота (recall) и F1-мера для предсказания "не покупок" выше 94%, а для проноза покупок близка к 90%.
